{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4192958573.py, line 180)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 180\u001b[1;36m\u001b[0m\n\u001b[1;33m    scaler = StandardScaler()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from fredapi import Fred\n",
    "from ta import add_all_ta_features\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from fracdiff.sklearn import FracdiffStat\n",
    "\n",
    "# Constants\n",
    "API_KEY = \"1e074c3898d6261abc56ff5962358644\"\n",
    "STOCK_TICKER = \"AAPL\"\n",
    "MARKET_TICKER = \"^GSPC\"\n",
    "START_DATE = \"2018-01-02\"\n",
    "END_DATE = \"2024-01-01\"\n",
    "SERIES_IDS = {\n",
    "    \"DGS10\": \"DGS10\",\n",
    "    \"VIXCLS\": \"VIXCLS\",\n",
    "    \"CRUDEOILWTI\": \"DCOILWTICO\",\n",
    "    \"DCOILBRENTEU\": \"DCOILBRENTEU\",\n",
    "    \"FFER\": \"DFF\",\n",
    "    \"USDollarIndex\": \"DTWEXBGS\",\n",
    "}\n",
    "\n",
    "\n",
    "class DataDownloader:\n",
    "    def __init__(self, api_key):\n",
    "        self.fred = Fred(api_key=api_key)\n",
    "\n",
    "    def safe_api_call(self, call, *args, **kwargs):\n",
    "        try:\n",
    "            return call(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"API call failed: {e}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "\n",
    "    def download_fred_series(self, series_id: str) -> pd.Series:\n",
    "        return self.safe_api_call(self.fred.get_series, series_id)\n",
    "\n",
    "    def download_and_process_yfinance_data(\n",
    "        self, ticker: str, start_date: str, end_date: str\n",
    "    ) -> pd.DataFrame:\n",
    "        data = self.safe_api_call(yf.download, ticker, start=start_date, end=end_date)\n",
    "        if not data.empty:\n",
    "            data.index = pd.DatetimeIndex(\n",
    "                data.index\n",
    "            ).normalize()  # Normalize to remove time\n",
    "            data = data.reindex(\n",
    "                pd.date_range(start=start_date, end=end_date, freq=\"B\"), method=\"ffill\"\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    def process_macroeconomic_data(\n",
    "        self, start_date: str, end_date: str\n",
    "    ) -> pd.DataFrame:\n",
    "        macro_df = pd.DataFrame(\n",
    "            index=pd.date_range(start=start_date, end=end_date, freq=\"B\")\n",
    "        )\n",
    "        for name, series_id in SERIES_IDS.items():\n",
    "            series = self.download_fred_series(series_id)\n",
    "            if not series.empty:\n",
    "                macro_df[name] = series.reindex(macro_df.index, method=\"ffill\")\n",
    "        return macro_df\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    @staticmethod\n",
    "    def preprocess_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        data = add_all_ta_features(\n",
    "            data,\n",
    "            open=\"Open\",\n",
    "            high=\"High\",\n",
    "            low=\"Low\",\n",
    "            close=\"Close\",\n",
    "            volume=\"Volume\",\n",
    "            fillna=True,\n",
    "        )\n",
    "        data[\"Target\"] = (data[\"Close\"].shift(-1) > data[\"Close\"]).astype(int)\n",
    "        return data.dropna()\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_min_max_normalization(data):\n",
    "        features = data.drop([\"Target\"], axis=1)\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_features = scaler.fit_transform(features)\n",
    "        normalized_columns = [f\"{col}_Norm\" for col in features.columns]\n",
    "        normalized_df = pd.DataFrame(\n",
    "            normalized_features, columns=normalized_columns, index=data.index\n",
    "        )\n",
    "        data_normalized = pd.concat([data, normalized_df], axis=1)\n",
    "        return data_normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_fractional_differencing(\n",
    "        df: pd.DataFrame, threshold: float = 0.05\n",
    "    ) -> pd.DataFrame:\n",
    "        f_diff = FracdiffStat()\n",
    "        for column in df.columns.difference([\"Target\"]):\n",
    "            p_value = adfuller(df[column])[1]\n",
    "            if p_value > threshold:\n",
    "                transformed_column = f_diff.fit_transform(df[[column]])\n",
    "                df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def haar_wavelet_denoising(series, level=1):\n",
    "        coeffs = pywt.wavedec(series, \"haar\", level=level)\n",
    "        coeffs[1:] = (\n",
    "            pywt.threshold(i, value=0.5 * np.max(i), mode=\"soft\") for i in coeffs[1:]\n",
    "        )\n",
    "        denoised_series = pywt.waverec(coeffs, \"haar\")\n",
    "        if len(denoised_series) > len(series):\n",
    "            denoised_series = denoised_series[: len(series)]\n",
    "        elif len(denoised_series) < len(series):\n",
    "            denoised_series = np.pad(\n",
    "                denoised_series, (0, len(series) - len(denoised_series)), \"edge\"\n",
    "            )\n",
    "        return denoised_series\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_haar_denoising(data):\n",
    "        denoised_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "        for column in denoised_columns:\n",
    "            denoised_series = DataPreprocessor.haar_wavelet_denoising(\n",
    "                data[column].values\n",
    "            )\n",
    "            data[f\"{column}_Denoised\"] = denoised_series\n",
    "        return data\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, n_splits=5):\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def time_series_cross_validation(self, X, y):\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        mean_scores = []\n",
    "\n",
    "        for n_features in range(1, X.shape[1] + 1):\n",
    "            scores = []\n",
    "            for train_index, test_index in tscv.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "                rfe = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "                          n_features_to_select=n_features)\n",
    "                rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "                X_train_selected = X_train_scaled[:, rfe.support_]\n",
    "                X_test_selected = X_test_scaled[:, rfe.support_]\n",
    "\n",
    "                model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "                model.fit(X_train_selected, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_test_selected)\n",
    "                scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "            mean_scores.append(np.mean(scores))\n",
    "\n",
    "        plt.plot(range(1, X.shape[1] + 1), mean_scores)\n",
    "        plt.xlabel(\"Number of Features\")\n",
    "        plt.ylabel(\"Mean Accuracy\")\n",
    "        plt.show()\n",
    "\n",
    "        return np.argmax(mean_scores) + 1\n",
    "\n",
    "    def train_and_evaluate_model(X_train, y_train, X_test, y_test, estimator, n_features_to_select):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    rfe = RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Extracting optimal feature names\n",
    "    feature_names = X_train.columns[rfe.support_]\n",
    "\n",
    "    X_train_selected = X_train_scaled[:, rfe.support_]\n",
    "    X_test_selected = X_test_scaled[:, rfe.support_]\n",
    "\n",
    "    model = clone(estimator)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"Optimal features: {feature_names}\")\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "    return model, feature_names, y_pred\n",
    "\n",
    "def calculate_trading_performance(stock_prices, predictions):\n",
    "    returns = stock_prices.pct_change().shift(-1)[:-1]  # Calculate returns for the investment period\n",
    "    strategy_returns = returns * predictions[:-1]  # Apply predictions to returns; align lengths\n",
    "\n",
    "    cumulative_return = (strategy_returns + 1).cumprod().iloc[-1] - 1\n",
    "    negative_returns = strategy_returns[strategy_returns < 0]\n",
    "    downside_deviation = np.sqrt(np.mean(negative_returns ** 2))\n",
    "\n",
    "    target_return = 0\n",
    "    sortino_ratio = (strategy_returns.mean() - target_return) / downside_deviation if downside_deviation else 0\n",
    "\n",
    "    print(f\"Cumulative Return: {cumulative_return}, Downside Deviation: {downside_deviation}, Sortino Ratio: {sortino_ratio}\")\n",
    "\n",
    "    return cumulative_return, downside_deviation, sortino_ratio\n",
    "\n",
    "\n",
    "\n",
    "downloader = DataDownloader(API_KEY)\n",
    "preprocessor = DataPreprocessor()\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "stock_data = downloader.download_and_process_yfinance_data(\n",
    "    STOCK_TICKER, START_DATE, END_DATE\n",
    ")\n",
    "market_data = downloader.download_and_process_yfinance_data(\n",
    "    MARKET_TICKER, START_DATE, END_DATE\n",
    ")\n",
    "macro_data = downloader.process_macroeconomic_data(START_DATE, END_DATE)\n",
    "\n",
    "additional_data = {\n",
    "    \"GoldPrices\": downloader.download_and_process_yfinance_data(\n",
    "        \"GC=F\", START_DATE, END_DATE\n",
    "    ),\n",
    "    \"OilPrices\": downloader.download_and_process_yfinance_data(\n",
    "        \"CL=F\", START_DATE, END_DATE\n",
    "    ),\n",
    "}\n",
    "\n",
    "combined_data = stock_data.join(\n",
    "    market_data[[\"Close\"]].rename(columns={\"Close\": \"MarketClose\"}), how=\"left\"\n",
    ").fillna(method=\"ffill\")\n",
    "combined_data = combined_data.join(macro_data, how=\"left\").fillna(method=\"ffill\")\n",
    "for name, data in additional_data.items():\n",
    "    combined_data = combined_data.join(\n",
    "        data[[\"Close\"]].rename(columns={\"Close\": f\"{name}Close\"}), how=\"left\"\n",
    "    ).fillna(method=\"ffill\")\n",
    "\n",
    "combined_data_denoised = preprocessor.apply_haar_denoising(combined_data)\n",
    "preprocessed_data = preprocessor.preprocess_data(combined_data_denoised)\n",
    "final_dataset = preprocessor.apply_fractional_differencing(preprocessed_data)\n",
    "final_dataset_normalized = preprocessor.apply_min_max_normalization(final_dataset)\n",
    "\n",
    "X = final_dataset_normalized.drop(\"Target\", axis=1)\n",
    "y = final_dataset_normalized[\"Target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "optimal_n_features = trainer.time_series_cross_validation(X_train, y_train)\n",
    "print(f\"Optimal number of features: {optimal_n_features}\")\n",
    "\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "model, feature_names, y_pred = train_and_evaluate_model(X_train, y_train, X_test, y_test, estimator, optimal_n_features)\n",
    "\n",
    "stock_prices_test = stock_data.loc[X_test.index, 'Close']\n",
    "\n",
    "calculate_trading_performance(stock_prices_test, y_pred)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
