{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\ta\\trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is 100% aligned with no NaN values.\n",
      "Shape of the DataFrame after aligning and cleaning: (1459, 103)\n",
      "            Fed Funds Rate  10-Yr Treasury    VIX  WTI Crude Oil  \\\n",
      "2018-01-03            1.42            2.44   9.15          61.61   \n",
      "2018-01-04            1.42            2.46   9.22          61.98   \n",
      "2018-01-05            1.42            2.47   9.22          61.49   \n",
      "2018-01-08            1.42            2.49   9.52          61.73   \n",
      "2018-01-09            1.42            2.55  10.08          62.92   \n",
      "\n",
      "            Brent Crude Oil  Trade Weighted USD Index       Open       High  \\\n",
      "2018-01-03            67.85                  109.6751  43.132500  43.637501   \n",
      "2018-01-04            68.73                  109.4779  43.134998  43.367500   \n",
      "2018-01-05            68.01                  109.3496  43.360001  43.842499   \n",
      "2018-01-08            68.48                  109.5678  43.587502  43.902500   \n",
      "2018-01-09            69.08                  109.9022  43.637501  43.764999   \n",
      "\n",
      "                  Low      Close  ...  momentum_pvo_hist  momentum_kama  \\\n",
      "2018-01-03  42.990002  43.057499  ...           0.978149      43.061529   \n",
      "2018-01-04  43.020000  43.257500  ...          -0.211168      43.152893   \n",
      "2018-01-05  43.262501  43.750000  ...          -0.652135      43.421203   \n",
      "2018-01-08  43.482498  43.587502  ...          -1.693583      43.495795   \n",
      "2018-01-09  43.352501  43.582500  ...          -2.013986      43.534185   \n",
      "\n",
      "            others_dr  others_dlr  others_cr  Target    return  Close_^GSPC  \\\n",
      "2018-01-03  -0.017415   -0.017416  -0.017415       1 -0.000174  2713.060059   \n",
      "2018-01-04   0.464497    0.463421   0.447001       1  0.004645  2723.989990   \n",
      "2018-01-05   1.138532    1.132099   1.590622       0  0.011385  2743.149902   \n",
      "2018-01-08  -0.371425   -0.372117   1.213289       0 -0.003714  2747.709961   \n",
      "2018-01-09  -0.011474   -0.011474   1.201676       0 -0.000115  2751.290039   \n",
      "\n",
      "             Close_GC=F  Close_CL=F  \n",
      "2018-01-03  1316.199951   61.630001  \n",
      "2018-01-04  1319.400024   62.009998  \n",
      "2018-01-05  1320.300049   61.439999  \n",
      "2018-01-08  1318.599976   61.730000  \n",
      "2018-01-09  1311.699951   62.959999  \n",
      "\n",
      "[5 rows x 103 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import yfinance as yf\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "# Setup\n",
    "fred_api_key = '1e074c3898d6261abc56ff5962358644'  # FRED API key\n",
    "fred = Fred(api_key=fred_api_key)\n",
    "stock_symbol = 'AAPL'  # Stock symbol for Apple Inc.\n",
    "\n",
    "# Define the economic indicators to fetch from FRED\n",
    "indicators = {\n",
    "    'DFF': 'Fed Funds Rate',\n",
    "    'DGS10': '10-Yr Treasury',\n",
    "    'VIXCLS': 'VIX',\n",
    "    'DCOILWTICO': 'WTI Crude Oil',\n",
    "    'DCOILBRENTEU': 'Brent Crude Oil',\n",
    "    'DTWEXBGS': 'Trade Weighted USD Index'\n",
    "}\n",
    "\n",
    "# Fetch Economic Indicators from FRED\n",
    "fred_data = pd.DataFrame()\n",
    "for code, name in indicators.items():\n",
    "    series = fred.get_series(code, observation_start='2018-01-01', observation_end='2024-01-01')\n",
    "    fred_data[name] = series\n",
    "\n",
    "fred_data.index = pd.to_datetime(fred_data.index)  # Ensure the index is datetime for easier alignment\n",
    "\n",
    "# Fetch Stock Data from Yahoo Finance\n",
    "aapl_data = yf.download(stock_symbol, start='2018-01-01', end='2024-01-01')\n",
    "\n",
    "aapl_data = add_all_ta_features(\n",
    "            aapl_data,\n",
    "            open=\"Open\",\n",
    "            high=\"High\",\n",
    "            low=\"Low\",\n",
    "            close=\"Close\",\n",
    "            volume=\"Volume\",\n",
    "            fillna=True,\n",
    "        )\n",
    "aapl_data[\"Target\"] = (aapl_data[\"Close\"].shift(-1) > aapl_data[\"Close\"]).astype(int)\n",
    "\n",
    "aapl_data[\"return\"] = aapl_data[\"Close\"].pct_change()\n",
    "\n",
    "aapl_data = aapl_data.dropna()  # Drop any rows with NaN values\n",
    "\n",
    "# Ensure the index is datetime for easier alignment\n",
    "aapl_data.index = pd.to_datetime(aapl_data.index)\n",
    "\n",
    "# Aligning Data by Date Without Filling Missing Data\n",
    "# To align all columns from AAPL data with FRED data, we merge on the index (date) without filling missing data\n",
    "aligned_data = pd.merge(fred_data, aapl_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# List of additional symbols to fetch from Yahoo Finance\n",
    "additional_symbols = ['^GSPC', 'GC=F', 'CL=F']\n",
    "\n",
    "# Fetch additional data and align it\n",
    "for symbol in additional_symbols:\n",
    "    # Fetch data for each symbol\n",
    "    symbol_data = yf.download(symbol, start='2018-01-01', end='2024-01-01')\n",
    "    # Ensure the index is datetime for easier alignment\n",
    "    symbol_data.index = pd.to_datetime(symbol_data.index)\n",
    "    # Merge with the aligned_data DataFrame\n",
    "    aligned_data = pd.merge(aligned_data, symbol_data[['Close']], left_index=True, right_index=True, how='inner', suffixes=('', f'_{symbol}'))\n",
    "\n",
    "\n",
    "# After merging all additional data, check for NaN values and drop any rows with NaN values to ensure 100% alignment\n",
    "aligned_data.dropna(inplace=True)\n",
    "\n",
    "# Validation\n",
    "if aligned_data.isnull().any().any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Data is 100% aligned with no NaN values.\")\n",
    "\n",
    "# Display the shape of the final DataFrame and a few rows to verify\n",
    "print(f\"Shape of the DataFrame after aligning and cleaning: {aligned_data.shape}\")\n",
    "print(aligned_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\ta\\trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "c:\\Users\\markm\\miniconda3\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is 100% aligned with no NaN values.\n",
      "Shape of the DataFrame after aligning and cleaning: (1459, 103)\n",
      "             DFF  DGS10  VIXCLS  DCOILWTICO  DCOILBRENTEU  DTWEXBGS  \\\n",
      "2018-01-03  1.42   2.44    9.15       61.61         67.85  109.6751   \n",
      "2018-01-04  1.42   2.46    9.22       61.98         68.73  109.4779   \n",
      "2018-01-05  1.42   2.47    9.22       61.49         68.01  109.3496   \n",
      "2018-01-08  1.42   2.49    9.52       61.73         68.48  109.5678   \n",
      "2018-01-09  1.42   2.55   10.08       62.92         69.08  109.9022   \n",
      "\n",
      "                 Open       High        Low      Close  ...  \\\n",
      "2018-01-03  43.132500  43.637501  42.990002  43.057499  ...   \n",
      "2018-01-04  43.134998  43.367500  43.020000  43.257500  ...   \n",
      "2018-01-05  43.360001  43.842499  43.262501  43.750000  ...   \n",
      "2018-01-08  43.587502  43.902500  43.482498  43.587502  ...   \n",
      "2018-01-09  43.637501  43.764999  43.352501  43.582500  ...   \n",
      "\n",
      "            momentum_pvo_hist  momentum_kama  others_dr  others_dlr  \\\n",
      "2018-01-03           0.978149      43.061529  -0.017415   -0.017416   \n",
      "2018-01-04          -0.211168      43.152893   0.464497    0.463421   \n",
      "2018-01-05          -0.652135      43.421203   1.138532    1.132099   \n",
      "2018-01-08          -1.693583      43.495795  -0.371425   -0.372117   \n",
      "2018-01-09          -2.013986      43.534185  -0.011474   -0.011474   \n",
      "\n",
      "            others_cr  Target    return        ^GSPC         GC=F       CL=F  \n",
      "2018-01-03  -0.017415       1 -0.000174  2713.060059  1316.199951  61.630001  \n",
      "2018-01-04   0.447001       1  0.004645  2723.989990  1319.400024  62.009998  \n",
      "2018-01-05   1.590622       0  0.011385  2743.149902  1320.300049  61.439999  \n",
      "2018-01-08   1.213289       0 -0.003714  2747.709961  1318.599976  61.730000  \n",
      "2018-01-09   1.201676       0 -0.000115  2751.290039  1311.699951  62.959999  \n",
      "\n",
      "[5 rows x 103 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from fredapi import Fred\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "def setup_fred(api_key):\n",
    "    \"\"\"Setup FRED API client.\"\"\"\n",
    "    return Fred(api_key=api_key)\n",
    "\n",
    "def fetch_fred_series(fred, series_code, start_date, end_date):\n",
    "    \"\"\"Fetch a time series from FRED.\"\"\"\n",
    "    try:\n",
    "        series = fred.get_series(series_code, observation_start=start_date, observation_end=end_date)\n",
    "        return pd.Series(series, name=series_code)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {series_code} from FRED: {e}\")\n",
    "        return pd.Series(name=series_code)\n",
    "\n",
    "def fetch_and_prepare_stock_data(symbol, start_date, end_date):\n",
    "    \"\"\"Fetch stock data from Yahoo Finance and prepare it with technical analysis features.\"\"\"\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data = add_all_ta_features(\n",
    "        stock_data,\n",
    "        open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True\n",
    "    )\n",
    "    stock_data[\"Target\"] = (stock_data[\"Close\"].shift(-1) > stock_data[\"Close\"]).astype(int)\n",
    "    stock_data[\"return\"] = stock_data[\"Close\"].pct_change()\n",
    "    return stock_data.dropna()\n",
    "\n",
    "def align_data(*dataframes):\n",
    "    \"\"\"Align multiple dataframes by their index.\"\"\"\n",
    "    aligned_df = pd.concat(dataframes, axis=1, join='inner')\n",
    "    aligned_df.dropna(inplace=True)  # Ensure no NaN values after alignment\n",
    "    return aligned_df\n",
    "\n",
    "# Configuration and setup\n",
    "fred_api_key = '1e074c3898d6261abc56ff5962358644'  # Replace with your FRED API key\n",
    "fred = setup_fred(fred_api_key)\n",
    "stock_symbol = 'AAPL'\n",
    "start_date, end_date = '2018-01-01', '2024-01-01'\n",
    "\n",
    "# Economic indicators to fetch\n",
    "indicators = ['DFF', 'DGS10', 'VIXCLS', 'DCOILWTICO', 'DCOILBRENTEU', 'DTWEXBGS']\n",
    "\n",
    "# Fetch and prepare economic indicators data\n",
    "fred_data = pd.concat([fetch_fred_series(fred, code, start_date, end_date) for code in indicators], axis=1)\n",
    "\n",
    "# Fetch and prepare stock data\n",
    "aapl_data = fetch_and_prepare_stock_data(stock_symbol, start_date, end_date)\n",
    "\n",
    "# Fetch additional data and align all\n",
    "additional_symbols = ['^GSPC', 'GC=F', 'CL=F']\n",
    "additional_data = [yf.download(symbol, start=start_date, end=end_date)['Close'].rename(symbol) for symbol in additional_symbols]\n",
    "\n",
    "# Aligning all data\n",
    "aligned_data = align_data(fred_data, aapl_data, *additional_data)\n",
    "\n",
    "# Validation and display\n",
    "if aligned_data.isnull().any().any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Data is 100% aligned with no NaN values.\")\n",
    "\n",
    "print(f\"Shape of the DataFrame after aligning and cleaning: {aligned_data.shape}\")\n",
    "print(aligned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
      "C:\\Users\\markm\\AppData\\Local\\Temp\\ipykernel_7796\\547635613.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_fdiff\"] = transformed_column.squeeze()\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from fracdiff.sklearn import FracdiffStat\n",
    "\n",
    "def apply_fractional_differencing(\n",
    "    df: pd.DataFrame, threshold: float = 0.05\n",
    ") -> pd.DataFrame:\n",
    "    f_diff = FracdiffStat()\n",
    "    for column in df.columns.difference([\"Target\"]):\n",
    "        p_value = adfuller(df[column])[1]\n",
    "        if p_value > threshold:\n",
    "            transformed_column = f_diff.fit_transform(df[[column]])\n",
    "            df[column + \"_fdiff\"] = transformed_column.squeeze()\n",
    "    return df\n",
    "\n",
    "# Apply fractional differencing\n",
    "aligned_data = apply_fractional_differencing(aligned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DFF  DGS10  VIXCLS  DCOILWTICO  DCOILBRENTEU  DTWEXBGS  \\\n",
      "2018-01-03  1.42   2.44    9.15       61.61         67.85  109.6751   \n",
      "2018-01-04  1.42   2.46    9.22       61.98         68.73  109.4779   \n",
      "2018-01-05  1.42   2.47    9.22       61.49         68.01  109.3496   \n",
      "2018-01-08  1.42   2.49    9.52       61.73         68.48  109.5678   \n",
      "2018-01-09  1.42   2.55   10.08       62.92         69.08  109.9022   \n",
      "\n",
      "                 Open       High        Low      Close  ...  \\\n",
      "2018-01-03  43.132500  43.637501  42.990002  43.057499  ...   \n",
      "2018-01-04  43.134998  43.367500  43.020000  43.257500  ...   \n",
      "2018-01-05  43.360001  43.842499  43.262501  43.750000  ...   \n",
      "2018-01-08  43.587502  43.902500  43.482498  43.587502  ...   \n",
      "2018-01-09  43.637501  43.764999  43.352501  43.582500  ...   \n",
      "\n",
      "            volatility_dcl_fdiff  volatility_dcm_fdiff  volatility_kcc_fdiff  \\\n",
      "2018-01-03             42.314999             42.976250             43.023333   \n",
      "2018-01-04              6.611719              5.707783              5.777925   \n",
      "2018-01-05              3.822400              3.335423              3.377729   \n",
      "2018-01-08              2.747350              2.342021              2.410866   \n",
      "2018-01-09              2.167831              1.811815              1.870312   \n",
      "\n",
      "            volatility_kch_fdiff  volatility_kcl_fdiff  volume_adi_fdiff  \\\n",
      "2018-01-03             43.727084             42.319583      6.077616e+06   \n",
      "2018-01-04              5.752642              6.464452      3.354252e+07   \n",
      "2018-01-05              3.413647              3.768274      6.813734e+07   \n",
      "2018-01-08              2.406654              2.738946     -3.250848e+07   \n",
      "2018-01-09              1.865912              2.139988      1.084449e+07   \n",
      "\n",
      "            volume_nvi_fdiff  volume_obv_fdiff  volume_vpt_fdiff  \\\n",
      "2018-01-03       1000.000000     -1.584800e+07     -2.056196e+04   \n",
      "2018-01-04         74.957469      8.515734e+07      4.107278e+05   \n",
      "2018-01-05         37.954773      1.176274e+08      1.197295e+06   \n",
      "2018-01-08         22.410638     -4.044810e+07      9.152047e+04   \n",
      "2018-01-09         19.790223     -8.158060e+07      1.657391e+05   \n",
      "\n",
      "            volume_vwap_fdiff  \n",
      "2018-01-03          43.038081  \n",
      "2018-01-04           7.448379  \n",
      "2018-01-05           4.466807  \n",
      "2018-01-08           3.239358  \n",
      "2018-01-09           2.559258  \n",
      "\n",
      "[5 rows x 144 columns]\n"
     ]
    }
   ],
   "source": [
    "print(aligned_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def apply_min_max_normalization(data):\n",
    "    features = data.drop([\"Target\"], axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_features = scaler.fit_transform(features)\n",
    "    normalized_columns = [f\"{col}_Norm\" for col in features.columns]\n",
    "    normalized_df = pd.DataFrame(\n",
    "        normalized_features, columns=normalized_columns, index=data.index\n",
    "    )\n",
    "    data_normalized = pd.concat([data, normalized_df], axis=1)\n",
    "    return data_normalized\n",
    "\n",
    "# Apply Min-Max normalization\n",
    "aligned_data_normalized = apply_min_max_normalization(aligned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DFF</th>\n",
       "      <th>DGS10</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>DCOILWTICO</th>\n",
       "      <th>DCOILBRENTEU</th>\n",
       "      <th>DTWEXBGS</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>...</th>\n",
       "      <th>volatility_dcl_fdiff_Norm</th>\n",
       "      <th>volatility_dcm_fdiff_Norm</th>\n",
       "      <th>volatility_kcc_fdiff_Norm</th>\n",
       "      <th>volatility_kch_fdiff_Norm</th>\n",
       "      <th>volatility_kcl_fdiff_Norm</th>\n",
       "      <th>volume_adi_fdiff_Norm</th>\n",
       "      <th>volume_nvi_fdiff_Norm</th>\n",
       "      <th>volume_obv_fdiff_Norm</th>\n",
       "      <th>volume_vpt_fdiff_Norm</th>\n",
       "      <th>volume_vwap_fdiff_Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.42</td>\n",
       "      <td>2.44</td>\n",
       "      <td>9.15</td>\n",
       "      <td>61.61</td>\n",
       "      <td>67.85</td>\n",
       "      <td>109.6751</td>\n",
       "      <td>43.132500</td>\n",
       "      <td>43.637501</td>\n",
       "      <td>42.990002</td>\n",
       "      <td>43.057499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439474</td>\n",
       "      <td>0.528494</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.42</td>\n",
       "      <td>2.46</td>\n",
       "      <td>9.22</td>\n",
       "      <td>61.98</td>\n",
       "      <td>68.73</td>\n",
       "      <td>109.4779</td>\n",
       "      <td>43.134998</td>\n",
       "      <td>43.367500</td>\n",
       "      <td>43.020000</td>\n",
       "      <td>43.257500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231973</td>\n",
       "      <td>0.172354</td>\n",
       "      <td>0.150158</td>\n",
       "      <td>0.145777</td>\n",
       "      <td>0.167278</td>\n",
       "      <td>0.509224</td>\n",
       "      <td>0.301977</td>\n",
       "      <td>0.523347</td>\n",
       "      <td>0.533631</td>\n",
       "      <td>0.176818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1.42</td>\n",
       "      <td>2.47</td>\n",
       "      <td>9.22</td>\n",
       "      <td>61.49</td>\n",
       "      <td>68.01</td>\n",
       "      <td>109.3496</td>\n",
       "      <td>43.360001</td>\n",
       "      <td>43.842499</td>\n",
       "      <td>43.262501</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171971</td>\n",
       "      <td>0.119670</td>\n",
       "      <td>0.095392</td>\n",
       "      <td>0.093162</td>\n",
       "      <td>0.104660</td>\n",
       "      <td>0.552931</td>\n",
       "      <td>0.274056</td>\n",
       "      <td>0.550310</td>\n",
       "      <td>0.542999</td>\n",
       "      <td>0.107855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1.42</td>\n",
       "      <td>2.49</td>\n",
       "      <td>9.52</td>\n",
       "      <td>61.73</td>\n",
       "      <td>68.48</td>\n",
       "      <td>109.5678</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>43.902500</td>\n",
       "      <td>43.482498</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148845</td>\n",
       "      <td>0.097609</td>\n",
       "      <td>0.073331</td>\n",
       "      <td>0.070510</td>\n",
       "      <td>0.080754</td>\n",
       "      <td>0.425776</td>\n",
       "      <td>0.262326</td>\n",
       "      <td>0.419046</td>\n",
       "      <td>0.529829</td>\n",
       "      <td>0.079465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>1.42</td>\n",
       "      <td>2.55</td>\n",
       "      <td>10.08</td>\n",
       "      <td>62.92</td>\n",
       "      <td>69.08</td>\n",
       "      <td>109.9022</td>\n",
       "      <td>43.637501</td>\n",
       "      <td>43.764999</td>\n",
       "      <td>43.352501</td>\n",
       "      <td>43.582500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136379</td>\n",
       "      <td>0.085834</td>\n",
       "      <td>0.060997</td>\n",
       "      <td>0.058346</td>\n",
       "      <td>0.066844</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>0.260349</td>\n",
       "      <td>0.384891</td>\n",
       "      <td>0.530713</td>\n",
       "      <td>0.063734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             DFF  DGS10  VIXCLS  DCOILWTICO  DCOILBRENTEU  DTWEXBGS  \\\n",
       "2018-01-03  1.42   2.44    9.15       61.61         67.85  109.6751   \n",
       "2018-01-04  1.42   2.46    9.22       61.98         68.73  109.4779   \n",
       "2018-01-05  1.42   2.47    9.22       61.49         68.01  109.3496   \n",
       "2018-01-08  1.42   2.49    9.52       61.73         68.48  109.5678   \n",
       "2018-01-09  1.42   2.55   10.08       62.92         69.08  109.9022   \n",
       "\n",
       "                 Open       High        Low      Close  ...  \\\n",
       "2018-01-03  43.132500  43.637501  42.990002  43.057499  ...   \n",
       "2018-01-04  43.134998  43.367500  43.020000  43.257500  ...   \n",
       "2018-01-05  43.360001  43.842499  43.262501  43.750000  ...   \n",
       "2018-01-08  43.587502  43.902500  43.482498  43.587502  ...   \n",
       "2018-01-09  43.637501  43.764999  43.352501  43.582500  ...   \n",
       "\n",
       "            volatility_dcl_fdiff_Norm  volatility_dcm_fdiff_Norm  \\\n",
       "2018-01-03                   1.000000                   1.000000   \n",
       "2018-01-04                   0.231973                   0.172354   \n",
       "2018-01-05                   0.171971                   0.119670   \n",
       "2018-01-08                   0.148845                   0.097609   \n",
       "2018-01-09                   0.136379                   0.085834   \n",
       "\n",
       "            volatility_kcc_fdiff_Norm  volatility_kch_fdiff_Norm  \\\n",
       "2018-01-03                   1.000000                   1.000000   \n",
       "2018-01-04                   0.150158                   0.145777   \n",
       "2018-01-05                   0.095392                   0.093162   \n",
       "2018-01-08                   0.073331                   0.070510   \n",
       "2018-01-09                   0.060997                   0.058346   \n",
       "\n",
       "            volatility_kcl_fdiff_Norm  volume_adi_fdiff_Norm  \\\n",
       "2018-01-03                   1.000000               0.474525   \n",
       "2018-01-04                   0.167278               0.509224   \n",
       "2018-01-05                   0.104660               0.552931   \n",
       "2018-01-08                   0.080754               0.425776   \n",
       "2018-01-09                   0.066844               0.480548   \n",
       "\n",
       "            volume_nvi_fdiff_Norm  volume_obv_fdiff_Norm  \\\n",
       "2018-01-03               1.000000               0.439474   \n",
       "2018-01-04               0.301977               0.523347   \n",
       "2018-01-05               0.274056               0.550310   \n",
       "2018-01-08               0.262326               0.419046   \n",
       "2018-01-09               0.260349               0.384891   \n",
       "\n",
       "            volume_vpt_fdiff_Norm  volume_vwap_fdiff_Norm  \n",
       "2018-01-03               0.528494                1.000000  \n",
       "2018-01-04               0.533631                0.176818  \n",
       "2018-01-05               0.542999                0.107855  \n",
       "2018-01-08               0.529829                0.079465  \n",
       "2018-01-09               0.530713                0.063734  \n",
       "\n",
       "[5 rows x 287 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_data_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = aligned_data_normalized.drop([\"Target\"], axis=1)\n",
    "y = aligned_data_normalized[\"Target\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=False, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 282\n",
      "Optimal features: ['DFF', 'DGS10', 'VIXCLS', 'DCOILWTICO', 'DCOILBRENTEU', 'DTWEXBGS', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'volume_adi', 'volume_obv', 'volume_cmf', 'volume_fi', 'volume_em', 'volume_sma_em', 'volume_vpt', 'volume_vwap', 'volume_mfi', 'volume_nvi', 'volatility_bbm', 'volatility_bbh', 'volatility_bbl', 'volatility_bbw', 'volatility_bbp', 'volatility_bbhi', 'volatility_kcc', 'volatility_kch', 'volatility_kcl', 'volatility_kcw', 'volatility_kcp', 'volatility_kchi', 'volatility_kcli', 'volatility_dcl', 'volatility_dch', 'volatility_dcm', 'volatility_dcw', 'volatility_dcp', 'volatility_atr', 'volatility_ui', 'trend_macd', 'trend_macd_signal', 'trend_macd_diff', 'trend_sma_fast', 'trend_sma_slow', 'trend_ema_fast', 'trend_ema_slow', 'trend_vortex_ind_pos', 'trend_vortex_ind_neg', 'trend_vortex_ind_diff', 'trend_trix', 'trend_mass_index', 'trend_dpo', 'trend_kst', 'trend_kst_sig', 'trend_kst_diff', 'trend_ichimoku_conv', 'trend_ichimoku_base', 'trend_ichimoku_a', 'trend_ichimoku_b', 'trend_stc', 'trend_adx', 'trend_adx_pos', 'trend_adx_neg', 'trend_cci', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_up', 'trend_aroon_down', 'trend_aroon_ind', 'trend_psar_up', 'trend_psar_down', 'trend_psar_up_indicator', 'trend_psar_down_indicator', 'momentum_rsi', 'momentum_stoch_rsi', 'momentum_stoch_rsi_k', 'momentum_stoch_rsi_d', 'momentum_tsi', 'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr', 'momentum_ao', 'momentum_roc', 'momentum_ppo', 'momentum_ppo_signal', 'momentum_ppo_hist', 'momentum_pvo', 'momentum_pvo_signal', 'momentum_pvo_hist', 'momentum_kama', 'others_dr', 'others_dlr', 'others_cr', 'return', '^GSPC', 'GC=F', 'CL=F', 'Adj Close_fdiff', 'CL=F_fdiff', 'Close_fdiff', 'DCOILBRENTEU_fdiff', 'DCOILWTICO_fdiff', 'DFF_fdiff', 'DGS10_fdiff', 'DTWEXBGS_fdiff', 'GC=F_fdiff', 'High_fdiff', 'Low_fdiff', 'Open_fdiff', '^GSPC_fdiff', 'momentum_kama_fdiff', 'others_cr_fdiff', 'trend_ema_fast_fdiff', 'trend_ema_slow_fdiff', 'trend_ichimoku_a_fdiff', 'trend_ichimoku_b_fdiff', 'trend_ichimoku_base_fdiff', 'trend_ichimoku_conv_fdiff', 'trend_psar_down_fdiff', 'trend_psar_up_fdiff', 'trend_sma_fast_fdiff', 'trend_sma_slow_fdiff', 'trend_visual_ichimoku_a_fdiff', 'trend_visual_ichimoku_b_fdiff', 'volatility_bbh_fdiff', 'volatility_bbl_fdiff', 'volatility_bbm_fdiff', 'volatility_dch_fdiff', 'volatility_dcl_fdiff', 'volatility_dcm_fdiff', 'volatility_kcc_fdiff', 'volatility_kch_fdiff', 'volatility_kcl_fdiff', 'volume_adi_fdiff', 'volume_nvi_fdiff', 'volume_obv_fdiff', 'volume_vpt_fdiff', 'volume_vwap_fdiff', 'DFF_Norm', 'DGS10_Norm', 'VIXCLS_Norm', 'DCOILWTICO_Norm', 'DCOILBRENTEU_Norm', 'DTWEXBGS_Norm', 'Open_Norm', 'High_Norm', 'Low_Norm', 'Close_Norm', 'Adj Close_Norm', 'Volume_Norm', 'volume_adi_Norm', 'volume_obv_Norm', 'volume_cmf_Norm', 'volume_fi_Norm', 'volume_em_Norm', 'volume_sma_em_Norm', 'volume_vpt_Norm', 'volume_vwap_Norm', 'volume_mfi_Norm', 'volume_nvi_Norm', 'volatility_bbm_Norm', 'volatility_bbh_Norm', 'volatility_bbl_Norm', 'volatility_bbw_Norm', 'volatility_bbp_Norm', 'volatility_kcc_Norm', 'volatility_kch_Norm', 'volatility_kcl_Norm', 'volatility_kcw_Norm', 'volatility_kcp_Norm', 'volatility_kcli_Norm', 'volatility_dcl_Norm', 'volatility_dch_Norm', 'volatility_dcm_Norm', 'volatility_dcw_Norm', 'volatility_dcp_Norm', 'volatility_atr_Norm', 'volatility_ui_Norm', 'trend_macd_Norm', 'trend_macd_signal_Norm', 'trend_macd_diff_Norm', 'trend_sma_fast_Norm', 'trend_sma_slow_Norm', 'trend_ema_fast_Norm', 'trend_ema_slow_Norm', 'trend_vortex_ind_pos_Norm', 'trend_vortex_ind_neg_Norm', 'trend_vortex_ind_diff_Norm', 'trend_trix_Norm', 'trend_mass_index_Norm', 'trend_dpo_Norm', 'trend_kst_Norm', 'trend_kst_sig_Norm', 'trend_kst_diff_Norm', 'trend_ichimoku_conv_Norm', 'trend_ichimoku_base_Norm', 'trend_ichimoku_a_Norm', 'trend_ichimoku_b_Norm', 'trend_stc_Norm', 'trend_adx_Norm', 'trend_adx_pos_Norm', 'trend_adx_neg_Norm', 'trend_cci_Norm', 'trend_visual_ichimoku_a_Norm', 'trend_visual_ichimoku_b_Norm', 'trend_aroon_up_Norm', 'trend_aroon_down_Norm', 'trend_aroon_ind_Norm', 'trend_psar_up_Norm', 'trend_psar_down_Norm', 'trend_psar_up_indicator_Norm', 'trend_psar_down_indicator_Norm', 'momentum_rsi_Norm', 'momentum_stoch_rsi_Norm', 'momentum_stoch_rsi_k_Norm', 'momentum_stoch_rsi_d_Norm', 'momentum_tsi_Norm', 'momentum_uo_Norm', 'momentum_stoch_Norm', 'momentum_stoch_signal_Norm', 'momentum_wr_Norm', 'momentum_ao_Norm', 'momentum_roc_Norm', 'momentum_ppo_Norm', 'momentum_ppo_signal_Norm', 'momentum_ppo_hist_Norm', 'momentum_pvo_Norm', 'momentum_pvo_signal_Norm', 'momentum_pvo_hist_Norm', 'momentum_kama_Norm', 'others_dr_Norm', 'others_dlr_Norm', 'others_cr_Norm', 'return_Norm', '^GSPC_Norm', 'GC=F_Norm', 'CL=F_Norm', 'Adj Close_fdiff_Norm', 'CL=F_fdiff_Norm', 'Close_fdiff_Norm', 'DCOILBRENTEU_fdiff_Norm', 'DCOILWTICO_fdiff_Norm', 'DFF_fdiff_Norm', 'DGS10_fdiff_Norm', 'DTWEXBGS_fdiff_Norm', 'GC=F_fdiff_Norm', 'High_fdiff_Norm', 'Low_fdiff_Norm', 'Open_fdiff_Norm', '^GSPC_fdiff_Norm', 'momentum_kama_fdiff_Norm', 'others_cr_fdiff_Norm', 'trend_ema_fast_fdiff_Norm', 'trend_ema_slow_fdiff_Norm', 'trend_ichimoku_a_fdiff_Norm', 'trend_ichimoku_b_fdiff_Norm', 'trend_ichimoku_base_fdiff_Norm', 'trend_ichimoku_conv_fdiff_Norm', 'trend_psar_down_fdiff_Norm', 'trend_psar_up_fdiff_Norm', 'trend_sma_fast_fdiff_Norm', 'trend_sma_slow_fdiff_Norm', 'trend_visual_ichimoku_a_fdiff_Norm', 'trend_visual_ichimoku_b_fdiff_Norm', 'volatility_bbh_fdiff_Norm', 'volatility_bbl_fdiff_Norm', 'volatility_bbm_fdiff_Norm', 'volatility_dch_fdiff_Norm', 'volatility_dcl_fdiff_Norm', 'volatility_dcm_fdiff_Norm', 'volatility_kcc_fdiff_Norm', 'volatility_kch_fdiff_Norm', 'volatility_kcl_fdiff_Norm', 'volume_adi_fdiff_Norm', 'volume_nvi_fdiff_Norm', 'volume_obv_fdiff_Norm', 'volume_vpt_fdiff_Norm', 'volume_vwap_fdiff_Norm']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rfecv = RFECV(estimator=rf, cv=tscv, scoring='accuracy')\n",
    "rfecv.fit(x_train, y_train)\n",
    "\n",
    "# Extracting optimal features\n",
    "optimal_features = x_train.columns[rfecv.support_]\n",
    "print(f\"Optimal number of features: {len(optimal_features)}\")\n",
    "print(f\"Optimal features: {list(optimal_features)}\")\n",
    "\n",
    "# Saving optimal features to CSV\n",
    "optimal_features_df = pd.DataFrame(optimal_features, columns=[\"Feature\"])\n",
    "optimal_features_df.to_csv(\"optimal_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4657534246575342,\n",
       " 0.6666666666666666,\n",
       " 0.05,\n",
       " 0.09302325581395349,\n",
       " array([[64,  2],\n",
       "        [76,  4]], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "y_pred = rfecv.predict(x_test)\n",
    "\n",
    "# Calculating performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy, precision, recall, f1, conf_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
